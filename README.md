# CVE Lakehouse on Databricks - Medallion Architecture

## Project Overview

A cybersecurity data lakehouse analyzing **32,924 CVE records from 2024** using Databricks and Delta Lake. Implements the Medallion Architecture (Bronze â†’ Silver â†’ Gold) for vulnerability intelligence.

**Course:** DIC 587 - Data Intensive Computing | Fall 2025

---

## Project Structure
```
dic_cve_assignmnet1/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ 2024_parquet.parquet          # CVE 2024 source data
â”‚
â”œâ”€â”€ source_code_as_per_submission/
â”‚   â”œâ”€â”€ 01_Bronze_Layer.py           
â”‚   â”œâ”€â”€ 02_Silver_Normalization.py   
â”‚   â”œâ”€â”€ 03_Gold_Analysis.py          
â”‚   â””â”€â”€ 03_Gold_Analysis.sql         
â”‚
â”œâ”€â”€ Screenshots/
â”‚   â”œâ”€â”€ 3rd_analysis.png             
â”‚   â”œâ”€â”€ 4th_analysis.png             
â”‚   â”œâ”€â”€ 5th_analysis.png             
â”‚   â”œâ”€â”€ 6th_analysis.png             
â”‚   â”œâ”€â”€ 7th_analysis.png             
â”‚   â””â”€â”€ 8th_analysis.png             
â”‚
â””â”€â”€ ipynb_for_clear_understanding/
    â”œâ”€â”€ 01_Bronze_Ingestion_proof.pdf
    â”œâ”€â”€ 02_Silver_Normalization_proof.pdf
    â””â”€â”€ 03_Exploratory_Analysis_ricks.pdf
```

---

## How to Run

### Step 1: Setup Databricks

1. Create account at https://community.cloud.databricks.com/
2. Create volume:
```sql
   CREATE VOLUME workspace.default.cve_lakehouse_data;
```
3. Upload `data/2024_parquet.parquet` to:  
   `/Volumes/workspace/default/cve_lakehouse_data/2024_parquet.parquet`

---

### Step 2: Import Notebooks

1. In Databricks, go to **Workspace**
2. Right-click â†’ **Import**
3. Import all files from `source_code_as_per_submission/`

---

### Step 3: Execute Pipeline

#### ðŸ¥‰ Bronze Layer (`01_Bronze_Layer.py`)

**Runtime:** ~2-3 minutes

**What it does:**
- Reads Parquet file
- Filters to 2024 CVEs
- Creates `cve_bronze.records` table (32,924 records)

**Run:** Click "Run All"

**Verify:**
```sql
SELECT COUNT(*) FROM cve_bronze.records;  -- Should return 32,924
```

---

#### ðŸ¥ˆ Silver Layer (`02_Silver_Normalization.py`)

**Runtime:** ~1-2 minutes

**What it does:**
- Normalizes CVE data
- Extracts core fields (dates, CVSS, descriptions)
- Explodes vendor/product arrays
- Creates 2 tables:
  - `cve_silver.core` (32,924 records)
  - `cve_silver.affected_products` (50,000+ records)

**Run:** Click "Run All"

**Verify:**
```sql
SELECT COUNT(*) FROM cve_silver.core;              -- 32,924
SELECT COUNT(*) FROM cve_silver.affected_products; -- 50,000+
```

---

#### ðŸ¥‡ Gold Layer (`03_Gold_Analysis.py` or `.sql`)

**Runtime:** <1 minute per query

**What it does:**
- 9 analytical queries:
  1. Yearly CVE trends
  2. Publication latency
  3. Monthly patterns
  4. CVSS risk distribution
  5. Top 25 vendors
  6. CVE state distribution
  7. Market concentration
  8. Monthly trends with CVSS
  9. Seasonal patterns

**Run:** Execute cells individually, use chart icons for visualizations

---

## Architecture

- **Bronze:** Raw data ingestion (JSON â†’ Delta)
- **Silver:** Normalized tables (core CVE + exploded products)
- **Gold:** Business analytics (9 security intelligence queries)

---

## Key Results

| Metric | Value |
|--------|-------|
| CVE Records (2024) | 32,924 |
| Unique Vendors | 1,500+ |
| Vendor/Product Combos | 50,000+ |
| CVEs with CVSS Scores | 85%+ |

---

## Technologies

- Databricks Community Edition (DBR 13.x+)
- Apache Spark / PySpark
- Delta Lake
- Python & SQL

---

## Troubleshooting

**"Table not found"**  
â†’ Run notebooks in order: 01 â†’ 02 â†’ 03

**"Path not found"**  
â†’ Verify parquet file uploaded to correct volume path

**"Volume does not exist"**  
â†’ Create volume first (see Step 1)

---

## Repository

**URL:** https://github.com/AnmolPatil2/dic_cve_assignmnet1  
**Course:** DIC 587 - Data Intensive Computing  
**Due Date:** November 16, 2025

---

**Data Source:** [CVEProject/cvelistV5](https://github.com/CVEProject/cvelistV5)
